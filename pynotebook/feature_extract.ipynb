{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import albumentations\n",
    "import torch\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load class & utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    \"resnet18\": \"https://download.pytorch.org/models/resnet18-f37072fd.pth\",\n",
    "    \"resnet34\": \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\",\n",
    "    \"resnet50\": \"https://download.pytorch.org/models/resnet50-19c8e357.pth\",\n",
    "    \"resnet101\": \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\",\n",
    "    \"resnet152\": \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\",\n",
    "}\n",
    "\n",
    "def l2_norm(input, axis=1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False\n",
    "    )\n",
    "\n",
    "class MixStyle(nn.Module):\n",
    "    \"\"\"Based on MixStyle.\n",
    "    https://github.com/KaiyangZhou/Dassl.pytorch/blob/master/dassl/modeling/ops/mixstyle.py\n",
    "    Reference:\n",
    "      Zhou et al. Domain Generalization with MixStyle. ICLR 2021.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5, alpha=0.1, eps=1e-6, mix=\"random\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          p (float): probability of using MixStyle.\n",
    "          alpha (float): parameter of the Beta distribution.\n",
    "          eps (float): scaling parameter to avoid numerical issues.\n",
    "          mix (str): how to mix.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.beta = torch.distributions.Beta(alpha, alpha)\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.mix = mix\n",
    "        self._activated = True\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"MixStyle(p={self.p}, alpha={self.alpha}, eps={self.eps}, mix={self.mix})\"\n",
    "        )\n",
    "\n",
    "    def set_activation_status(self, status=True):\n",
    "        self._activated = status\n",
    "\n",
    "    def update_mix_method(self, mix=\"random\"):\n",
    "        self.mix = mix\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        if not self.training or not self._activated:\n",
    "            return x\n",
    "\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "\n",
    "        B = x.size(0)\n",
    "\n",
    "        mu = x.mean(dim=[2, 3], keepdim=True)\n",
    "        var = x.var(dim=[2, 3], keepdim=True)\n",
    "        sig = (var + self.eps).sqrt()\n",
    "        mu, sig = mu.detach(), sig.detach()\n",
    "        x_normed = (x-mu) / sig\n",
    "\n",
    "        lmda = self.beta.sample((B, 1, 1, 1))\n",
    "        lmda = lmda.to(x.device)\n",
    "\n",
    "        if self.mix == \"random\":\n",
    "            # random shuffle\n",
    "            perm = torch.randperm(B)\n",
    "\n",
    "        elif self.mix == \"crossdomain\":\n",
    "            # split into two halves and swap the order\n",
    "            perm = torch.arange(B - 1, -1, -1)  # inverse index\n",
    "            perm_b, perm_a = perm.chunk(2)\n",
    "            perm_b = perm_b[torch.randperm(perm_b.shape[0])]\n",
    "            perm_a = perm_a[torch.randperm(perm_a.shape[0])]\n",
    "            perm = torch.cat([perm_b, perm_a], 0)\n",
    "        #######################\n",
    "        #        Added\n",
    "        #######################\n",
    "        elif self.mix == \"crosssample\":\n",
    "            assert labels != None, 'Label is None'\n",
    "            contrast_3d = (labels.long()  == 0).nonzero(as_tuple=True)[0]  # find 3d mask attack\n",
    "            contrast_bf = (labels.long() == 1).nonzero(as_tuple=True)[0] # find bonafide\n",
    "            contrast_print = (labels.long() == 2).nonzero(as_tuple=True)[0] # find print attack\n",
    "            contrast_cut = (labels.long() == 3).nonzero(as_tuple=True)[0] # find paper cut attack\n",
    "            contrast_replay = (labels.long() == 4).nonzero(as_tuple=True)[0] # find replay attack\n",
    "\n",
    "            perm_idx_3d = contrast_3d[torch.randperm(len(contrast_3d))]\n",
    "            perm_idx_bf = contrast_bf[torch.randperm(len(contrast_bf))]\n",
    "            perm_idx_print = contrast_print[torch.randperm(len(contrast_print))]\n",
    "            perm_idx_cut = contrast_cut[torch.randperm(len(contrast_cut))]\n",
    "            perm_idx_replay = contrast_replay[torch.randperm(len(contrast_replay))]\n",
    "\n",
    "            old_idx = torch.cat([contrast_bf, contrast_3d, contrast_print, contrast_cut, contrast_replay], 0)\n",
    "            perm = torch.cat([perm_idx_bf, perm_idx_3d, perm_idx_print, perm_idx_cut, perm_idx_replay], 0)\n",
    "            perm = perm[torch.argsort(old_idx)]\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        mu2, sig2 = mu[perm], sig[perm]\n",
    "        mu_mix = mu*lmda + mu2 * (1-lmda)\n",
    "        sig_mix = sig*lmda + sig2 * (1-lmda)\n",
    "\n",
    "        return x_normed*sig_mix + mu_mix\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes,\n",
    "            planes,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            planes, planes * self.expansion, kernel_size=1, bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def out_features(self):\n",
    "        \"\"\"Output feature dimension.\"\"\"\n",
    "        if self.__dict__.get(\"_out_features\") is None:\n",
    "            return None\n",
    "        return self._out_features\n",
    "\n",
    "\n",
    "class ResNet(Backbone):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block,\n",
    "        layers,\n",
    "        ms_class=None,\n",
    "        ms_layers=[],\n",
    "        ms_p=0.5,\n",
    "        ms_a=0.1,\n",
    "        mix=\"crosssample\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.inplanes = 64\n",
    "        super().__init__()\n",
    "\n",
    "        # backbone network\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        #self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self._out_features = 512 * block.expansion\n",
    "\n",
    "        self.mixstyle = None\n",
    "        if ms_layers:\n",
    "            self.mixstyle = ms_class(p=ms_p, alpha=ms_a, mix=mix)\n",
    "            for layer_name in ms_layers:\n",
    "                assert layer_name in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "            print(\n",
    "                f\"Insert {self.mixstyle.__class__.__name__} after {ms_layers}\"\n",
    "            )\n",
    "            print(f'Using {mix}')\n",
    "        else:\n",
    "            print('No MixStyle')\n",
    "        self.ms_layers = ms_layers\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.inplanes,\n",
    "                    planes * block.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode=\"fan_out\", nonlinearity=\"relu\"\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def featuremaps(self, x, labels=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        if \"layer1\" in self.ms_layers:\n",
    "            x = self.mixstyle(x, labels)\n",
    "\n",
    "        x = self.layer2(x)\n",
    "        if \"layer2\" in self.ms_layers:\n",
    "            x = self.mixstyle(x, labels)\n",
    "\n",
    "        x = self.layer3(x)\n",
    "        if \"layer3\" in self.ms_layers:\n",
    "            x = self.mixstyle(x, labels)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        if \"layer4\" in self.ms_layers:\n",
    "            x = self.mixstyle(x, labels)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        f = self.featuremaps(x, labels)\n",
    "        return f\n",
    "\n",
    "\n",
    "def init_pretrained_weights(model, model_url):\n",
    "    pretrain_dict = torch.hub.load_state_dict_from_url(model_url)\n",
    "    model.load_state_dict(pretrain_dict, strict=False)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels=512, num_classes=2):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.classifier_layer = nn.Linear(in_channels, num_classes)\n",
    "        self.classifier_layer.weight.data.normal_(0, 0.01)\n",
    "        self.classifier_layer.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, input, norm_flag=False):\n",
    "        if(norm_flag):\n",
    "            self.classifier_layer.weight.data = l2_norm(self.classifier_layer.weight, axis=0)\n",
    "            output = self.classifier_layer(input)\n",
    "        else:\n",
    "            output = self.classifier_layer(input)\n",
    "        return output\n",
    "\n",
    "class FeatExt_MixStyleResCausalModel(nn.Module):\n",
    "    def __init__(self, model_name='resnet18', pretrained=False, num_classes=2, prob=0.2, ms_class=MixStyle, ms_layers=[\"layer1\", \"layer2\"], mix=\"crosssample\"):\n",
    "        super(FeatExt_MixStyleResCausalModel, self).__init__()\n",
    "        self.feature_extractor = ResNet(\n",
    "            block=BasicBlock,\n",
    "            layers=[2, 2, 2, 2],\n",
    "            ms_class=ms_class,\n",
    "            ms_layers=ms_layers, #[\"layer1\", \"layer2\"], # \"layer3\", \"layer4\"\n",
    "            mix=mix\n",
    "        )\n",
    "        if pretrained:\n",
    "            print('-------load model----------')\n",
    "            init_pretrained_weights(self.feature_extractor, model_urls[model_name])\n",
    "        \n",
    "        self.classifier = Classifier(in_channels=512, num_classes=num_classes)   # resnet\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "    def forward(self, input, labels=None, cf=['cs', 'dropout', 'replace'], norm=True):\n",
    "        # labels = (labels == 1) + 0 # for handle multiclass\n",
    "        feature = self.feature_extractor(input, labels)  # Batch, 512, 7, 7\n",
    "\n",
    "        cls_feature = self.avgpool(feature)\n",
    "        cls_feature = cls_feature.view(cls_feature.size(0), -1)\n",
    "        cls = self.classifier(cls_feature)\n",
    "\n",
    "        if (not self.training) or cf is None or labels is None:\n",
    "            return cls, cls_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE__MEAN = [0.485, 0.456, 0.406]\n",
    "PRE__STD = [0.229, 0.224, 0.225]\n",
    "INPUT__FACE__SIZE = 256\n",
    "PADDING = 0\n",
    "\n",
    "def preprocess_frame_pipe():\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(height=INPUT__FACE__SIZE, width=INPUT__FACE__SIZE),\n",
    "        albumentations.Normalize(PRE__MEAN, PRE__STD, always_apply=True),\n",
    "        ToTensorV2(),    \n",
    "    ])\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = preprocess_frame_pipe()(image=frame)['image']\n",
    "    frame = torch.tensor(frame).unsqueeze(0)\n",
    "    return frame\n",
    "\n",
    "def get_feature(img_path, model):\n",
    "    device = next(model.parameters()).device\n",
    "    face = cv2.imread(img_path)\n",
    "    with torch.no_grad():\n",
    "        input_frame = preprocess_frame(face).to(device)\n",
    "        output, cls_feature = model(input_frame, cf=None)\n",
    "    return cls_feature\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = ''\n",
    "df_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda')\n",
    "else :\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "set_seed(seed=777)\n",
    "\n",
    "model = torch.nn.DataParallel(FeatExt_MixStyleResCausalModel(model_name='resnet18',  pretrained=False, num_classes=args.num_classes, ms_layers=[]))\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv()\n",
    "df['vector'] = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df.iterrows()):\n",
    "    path = df.iloc[index,0]  \n",
    "    feature = get_feature(path, model) \n",
    "    df['vector'] = feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
